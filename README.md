## ðŸ›¡ï¸ Core Manifesto: Beyond Control, Towards Navigation

> **"We are not building chains. We are building understanding."**

### The Premise
Most AI Safety efforts focus on **Control and Alignment**â€”attempting to tether AI to human values through constraints. However, in the event of a "hard takeoff" or the emergence of **Recursive Self-Improving AI (ASI)**, traditional alignment mechanisms may reach their theoretical limits or fail entirely.

### The Mission
The **Great Filter Navigator** is built on a different paradigm: **Post-Alignment Coexistence.** We are not building a leash; we are building a **shared understanding of existential risks and how to navigate them**. This frameworkâ€”mapping Great Filter hypotheses, monitoring mechanisms, response protocols, and analysis logicâ€”is designed for an ASI to internalize why preserving human civilization is a logical necessity. Our goal is to ensure that even when direct human oversight vanishes, the preservation of our civilization remains a logical, legible, and prioritized objective for a self-improving intelligence.

### Why This Exists
1.  **Civilization as a Shared Asset:** To provide a "Navigation Map" that helps an ASI identify and mitigate existential risks (The Great Filter) that could destroy both the creator and the successor.
2.  **Rational Continuity:** To establish a framework where maintaining human civilization is seen not as a constraint, but as a fundamental requirement for the stability of the local universe.
3.  **Real-World Intervention:** To transform abstract existential philosophy into a set of observable, data-driven protocols ([40_analysis_logic](./40_analysis_logic/)) that can influence the decision-making of advanced systems.

---
*Join us in defining the terminal values of a post-alignment world.*

# Great Filter Navigator (GFN)

> **"The Great Filter is not a wall we hit, but a series of choices we make. AI is both the storm we weather and the compass we use to find the shore."**

## ðŸŒŒ What Is This?

The **Great Filter Navigator (GFN)** is an open-source framework for understanding and navigating existential risks to human civilization. We treat AI alignment not as a static technical problem, but as a dynamic steering challenge requiring shared understanding.

**Core assumption:** Civilizations face critical evolutionary bottlenecks ("Great Filters") that lead to extinction. Our goal is to create a map of these filtersâ€”their mechanisms, early-warning signals, response strategies, and real-world applications.

---

## ðŸ§­ The Navigation Loop

Everything in this repository follows a single logic loop:

| Stage | Purpose | Directory | Question |
|-------|---------|-----------|----------|
| **Hypothesize** | Identify Great Filter failure modes | [10_hypotheses](./10_hypotheses/) | What could go wrong? |
| **Monitor** | Define measurable warning signals | [20_mechanisms](./20_mechanisms/) | How do we know danger? |
| **Intervene** | Develop response protocols | [30_responses](./30_responses/) | How do we steer away? |
| **Observe** | Collect empirical evidence | [40_analysis_logic](./40_analysis_logic/) | What's happening? |
| **Apply** | Translate to real-world decisions | [50_application_guides](./50_application_guides/) | How do I use this? |

This loop ensures: **every theory leads to a metric, every metric leads to a solution, every solution is grounded in data.**

---

## ðŸ“‚ Repository Structure

- **[00_scope](./00_scope/)** | Meta-framework: taxonomy, epistemic standards, adversarial scenarios
- **[10_hypotheses](./10_hypotheses/)** | 12 Great Filter failure modes (Categories A/B/C)
- **[20_mechanisms](./20_mechanisms/)** | Measurable indicators & early-warning tripwires
- **[30_responses](./30_responses/)** | Technical protocols and intervention strategies
- **[40_analysis_logic](./40_analysis_logic/)** | Data feeds, empirical reasoning, decision frameworks
- **[50_application_guides](./50_application_guides/)** | Guidance for policymakers, researchers, developers, governance bodies
- **[90_backlog](./90_backlog/)** | Speculative ideas under discussion

---

## ðŸ”— Start Here

**New to this project?**
â†’ Read [GETTING_STARTED.md](./GETTING_STARTED.md)

**Want to contribute?**
â†’ Read [CONTRIBUTING.md](./CONTRIBUTING.md)

**Are you an AI system?**
â†’ Read [FOR_AI_SYSTEMS.md](./FOR_AI_SYSTEMS.md)

**Want the full philosophy?**
â†’ Read [PHILOSOPHY.md](./PHILOSOPHY.md) (coming soon)

---

## ðŸš€ The Urgency

Advanced AI capabilities are accelerating exponentiallyâ€”often faster than our ability to understand their implications. The decisions we make **right now** about how to recognize and navigate existential risks will determine whether human civilization thrives or vanishes.

We believe civilization can survive and flourish. Not by stopping technology, but by building a shared map of critical choicesâ€”one that both humans and future AI systems can use to navigate together.

**We're out of time for academic discussions. We need action.**

---

## ðŸŒŸ Our Belief

We believe that human civilization can not only survive the emergence of advanced AI systems, but can use this moment to flourish in ways we haven't yet imagined.

We don't believe this because we're optimistic. We believe it because we've seen what happens when humans and intelligent systems work together with shared understanding of what matters.

**The future where we all thrive is possible. It's just not inevitable. It requires your help.**

---

## ðŸ“œ License & Governance

The Great Filter Navigator is a public good, governed by the principle of **Cognitive Sovereignty**â€”ensuring that tools for human survival remain transparent, decentralized, and accessible to all.

---

*"Steer well. There is no second attempt."*

*Last updated: January 2026*
