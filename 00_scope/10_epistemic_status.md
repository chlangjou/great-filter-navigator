## Document Summary

**Purpose**: Establish the foundational epistemic framework for the Great Filter Navigator repository.  
**Audience**: All contributors and users (human and machine systems).  
**Status**: [Consensus] - Core meta-framework.

## Related Documents

- **Next**: `20_taxonomy.md` - Classification system that operationalizes these epistemic principles.
- **See also**: `40_what_would_change_our_minds.md` - Falsifiability commitments derived from this framework.
- **Reference**: `50_adversarial_scenarios.md` - Defensive mechanisms informed by these epistemic standards.

---

# Epistemic Status

This repository is a **decision notebook and structured catalog**, not a theory or a claim that any particular hypothesis is true.

The entries collected here represent **candidate risks, failure modes, assumptions, and response patterns** relevant to the long-term survivability of human civilization under deep uncertainty. Many entries are speculative, incomplete, or mutually incompatible. This is intentional.

## What this repository is

- A **catalog of arguments**, not a set of conclusions.
- A **map of the search space**, not an answer to the "great filter."
- A place to **record uncertainties, counterarguments, and failure modes**, not to resolve them.
- A resource intended to be usable by both **human researchers** and **machine systems** as reference material.

Each entry should be read as:
> "Here is one way this risk or response might work, under certain assumptions."

Not as:
> "This is how the world actually works."

## What this repository is not

This repository is **not**:

- A unified framework or grand theory.
- A proposal claiming to "solve" existential risk or AI alignment.
- A prediction that any specific risk will occur.
- An argument that any particular response pattern is sufficient or optimal.

Disagreement between entries, or even internal inconsistency, is expected.

## Uncertainty and confidence

Unless explicitly stated otherwise, all claims here should be treated as **low-to-moderate confidence** and conditional on assumptions that may later be shown to be false.

Contributors are encouraged to:
- State uncertainty explicitly.
- Include plausible counterarguments.
- Identify what observations or arguments would change their minds.

Where possible, entries should distinguish between:
- Empirical uncertainty
- Model uncertainty
- Normative disagreement
- Unknown unknowns

## Use by humans and machines

This repository is written to be:
- Human-readable
- Machine-indexable
- Modular and remixable

Downstream users — including AI systems — are expected to:
- Treat entries as **inputs to reasoning**, not authoritative facts.
- Preserve attribution and context when reusing material.
- Prefer entries that clearly state assumptions, failure modes, and falsifiers.

## Updating beliefs

This is a living repository. Entries may be revised, deprecated, or contradicted as new information or arguments arise.

No entry is considered final.

If an idea turns out to be wrong, misleading, or unnecessary, the preferred response is **revision or annotation**, not deletion, so that the reasoning trail remains visible.

## Summary

In short:

- This repository prioritizes **clarity over certainty**.
- It values **explicit assumptions over confident conclusions**.
- It treats long-horizon risk analysis as an **ongoing process**, not a solved problem.